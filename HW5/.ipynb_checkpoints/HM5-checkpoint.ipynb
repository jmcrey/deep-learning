{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 5: Build a seq2seq model for machine translation.\n",
    "\n",
    "### Name: Jeremiah McReynolds\n",
    "\n",
    "### Task: Translate English to Russian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run my code.\n",
    "2. Complete the code in Section 1.1 and Section 4.2.\n",
    "\n",
    "    * Translation **English** to **German** is not acceptable!!! Try another pair of languages.\n",
    "    \n",
    "3. **Make improvements.** Directly modify the code in Section 3. Do at least one of the two. By doing both correctly, you will get up to 1 bonus score to the total.\n",
    "\n",
    "    * Bi-LSTM instead of LSTM.\n",
    "        \n",
    "    * Attention. (You are allowed to use existing code.)\n",
    "    \n",
    "4. Evaluate the translation using the BLEU score. \n",
    "\n",
    "    * Optional. Up to 1 bonus scores to the total.\n",
    "    \n",
    "5. Convert the notebook to .HTML file. \n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "\n",
    "6. Put the .HTML file in your Google Drive, Dropbox, or Github repo.  (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "7. Submit the link to the HTML file to Canvas.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "\n",
    "To implement ```Bi-LSTM```, you will need the following code to build the encoder. Do NOT use Bi-LSTM for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate\n",
    "\n",
    "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "                                  dropout=0.5, name='encoder_lstm'))\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "1. Download data (e.g., \"deu-eng.zip\") from http://www.manythings.org/anki/\n",
    "2. Unzip the .ZIP file.\n",
    "3. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load and clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('utf8')\n",
    "            line = line.decode('utf8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            # line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the following blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename = 'Data/rus.txt'\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# split into Language1-Language2 pairs\n",
    "pairs = to_pairs(doc)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_data(pairs)[0:n_train, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tom danced] => [том станцевал]\n",
      "[tom dances] => [том танцует]\n",
      "[tom did it] => [том сделал это]\n",
      "[tom did it] => [том это сделал]\n",
      "[tom drinks] => [том]\n",
      "[tom drives] => [том водит машину]\n",
      "[tom failed] => [у тома ничего не вышло]\n",
      "[tom failed] => [том потерпел фиаско]\n",
      "[tom failed] => [том потерпел неудачу]\n",
      "[tom failed] => [том не имел успеха]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (20000,)\n",
      "Length of target_texts: (20000,)\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input  sentences: 15\n",
      "max length of target sentences: 45\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (20000, 15)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (20000, 45)\n",
      "shape of target_token_index: 44\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "\n",
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "print('shape of target_token_index: ' + str(len(target_token_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 45\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
    "\n",
    "- Their number of rows are both n_train.\n",
    "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings print a sentence and its representation as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tидите\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  9, 19,  9,  5,  6,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq[100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. One-hot encode\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
    "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 15, 28)\n",
      "(20000, 45, 45)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('go now',\n",
       " array([17,  4,  1,  8,  4, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       dtype=int32),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[100], encoder_input_seq[100, :], encoder_input_data[100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the networks (for training)\n",
    "\n",
    "- Build encoder, decoder, and connect the two modules to get \"model\". \n",
    "\n",
    "- Fit the model on the bilingual data to train the parameters in the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Encoder network\n",
    "\n",
    "- Input:  one-hot encode of the input language\n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states   $h_1, \\cdots , h_t$) are always discarded\n",
    "    \n",
    "    -- the final hidden state  $h_t$\n",
    "    \n",
    "    -- the final conveyor belt $c_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Bidirectional, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "                    dropout=0.5, name='encoder_lstm'))\n",
    "encoder_out, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=[state_h, state_c],\n",
    "                      name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, \n",
    "                    dropout=0.5, name='encoder_lstm')\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=[state_h, state_c],\n",
    "                      name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary and save the encoder network structure to \"./encoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None, 28)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 512), (None, 583680      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 583,680\n",
      "Trainable params: 583,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=encoder_model, show_shapes=False,\n",
    "    to_file='encoder.pdf'\n",
    ")\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decoder network\n",
    "\n",
    "- Inputs:  \n",
    "\n",
    "    -- one-hot encode of the target language\n",
    "    \n",
    "    -- The initial hidden state $h_t$ \n",
    "    \n",
    "    -- The initial conveyor belt $c_t$ \n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
    "\n",
    "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
    "    \n",
    "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "# set the Attention layer\n",
    "attn_h, attn_c = Attention(name='attn_layer')([decoder_input_h, state_h])\n",
    "decoder_attn_out = Concatenate(axis=-1, name='attn_concat')([decoder_lstm_out, attn_h])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs, state_h, state_c],\n",
    "                      name='decoder')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense, Attention, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_outs = Input(shape=(None, latent_dim * 2), name='decoder_input_outs')\n",
    "decoder_input_h = Input(shape=(latent_dim * 2,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim * 2,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "# set the Attention layer\n",
    "# attn_h, attn_c = Attention(name='attn_layer')([decoder_input_outputs, decoder_lstm_outputs])\n",
    "# decoder_attn_out = Concatenate(axis=-1, name='attn_concat')([decoder_lstm_out, attn_h])\n",
    "\n",
    "# set the Dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs, state_h, state_c],\n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary and save the encoder network structure to \"./decoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    [(None, None, 45)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1142784     decoder_input_x[0][0]            \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 45)     23085       decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,165,869\n",
      "Trainable params: 1,165,869\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=decoder_model, show_shapes=False,\n",
    "    to_file='decoder.pdf'\n",
    ")\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_training\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    [(None, None, 28)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x (InputLayer)    [(None, None, 45)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            [(None, 512), (None, 583680      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1142784     decoder_input_x[0][0]            \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 encoder[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 45)     23085       decoder_lstm[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,749,549\n",
      "Trainable params: 1,749,549\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=model, show_shapes=False,\n",
    "    to_file='model_training.pdf'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Fit the model on the bilingual dataset\n",
    "\n",
    "- encoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_target_data: labels (left shift of decoder_input_data)\n",
    "\n",
    "- tune the hyper-parameters\n",
    "\n",
    "- stop when the validation loss stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_data(20000, 15, 28)\n",
      "shape of decoder_input_data(20000, 45, 45)\n",
      "shape of decoder_target_data(20000, 45, 45)\n"
     ]
    }
   ],
   "source": [
    "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 7s 9ms/step - loss: 1.2032 - val_loss: 0.7682\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.7552 - val_loss: 0.6901\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.6972 - val_loss: 0.6397\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.6489 - val_loss: 0.6095\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.6240 - val_loss: 0.5784\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.6033 - val_loss: 0.5559\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5834 - val_loss: 0.5415\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5691 - val_loss: 0.5269\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5499 - val_loss: 0.5169\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5348 - val_loss: 0.5034\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5283 - val_loss: 0.4906\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5163 - val_loss: 0.4809\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5072 - val_loss: 0.4759\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4922 - val_loss: 0.4670\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.4912 - val_loss: 0.4658\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4797 - val_loss: 0.4565\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4739 - val_loss: 0.4542\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4629 - val_loss: 0.4490\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4605 - val_loss: 0.4478\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.4577 - val_loss: 0.4432\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.4472 - val_loss: 0.4457\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4392 - val_loss: 0.4407\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4386 - val_loss: 0.4327\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4278 - val_loss: 0.4326\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4254 - val_loss: 0.4331\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.4219 - val_loss: 0.4336\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.4116 - val_loss: 0.4293\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4135 - val_loss: 0.4286\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4053 - val_loss: 0.4264\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4013 - val_loss: 0.4280\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3977 - val_loss: 0.4243\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3954 - val_loss: 0.4262\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3907 - val_loss: 0.4202\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3893 - val_loss: 0.4253\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3824 - val_loss: 0.4172\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3796 - val_loss: 0.4184\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3744 - val_loss: 0.4209\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.3767 - val_loss: 0.4210\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3687 - val_loss: 0.4153\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.3663 - val_loss: 0.4163\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3691 - val_loss: 0.4218\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3588 - val_loss: 0.4174\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3594 - val_loss: 0.4159\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3566 - val_loss: 0.4186\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.3533 - val_loss: 0.4166\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3485 - val_loss: 0.4174\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3515 - val_loss: 0.4171\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3463 - val_loss: 0.4203\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.3458 - val_loss: 0.4190\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.3386 - val_loss: 0.4196\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
    "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
    "          batch_size=64, epochs=50, validation_split=0.2)\n",
    "\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make predictions\n",
    "\n",
    "\n",
    "### 4.1. Translate English to Russian\n",
    "\n",
    "1. Encoder read a sentence (source language) and output its final states, $h_t$ and $c_t$.\n",
    "2. Take the [star] sign \"\\t\" and the final state $h_t$ and $c_t$ as input and run the decoder.\n",
    "3. Get the new states and predicted probability distribution.\n",
    "4. sample a char from the predicted probability distribution\n",
    "5. take the sampled char and the new states as input and repeat the process (stop if reach the [stop] sign \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        follow him\n",
      "Russian (true):  иди за ним\n",
      "Russian (pred):  за ним\n",
      "-\n",
      "English:        follow him\n",
      "Russian (true):  идите за ним\n",
      "Russian (pred):  за ним\n",
      "-\n",
      "English:        follow him\n",
      "Russian (true):  за ним\n",
      "Russian (pred):  за ним\n",
      "-\n",
      "English:        forget tom\n",
      "Russian (true):  забудь тома\n",
      "Russian (pred):  забудьте тома\n",
      "-\n",
      "English:        forget tom\n",
      "Russian (true):  забудьте тома\n",
      "Russian (pred):  забудьте тома\n",
      "-\n",
      "English:        forget him\n",
      "Russian (true):  забудь его\n",
      "Russian (pred):  забудь его\n",
      "-\n",
      "English:        forget him\n",
      "Russian (true):  забудьте его\n",
      "Russian (pred):  забудь его\n",
      "-\n",
      "English:        forgive me\n",
      "Russian (true):  простите меня\n",
      "Russian (pred):  простите меня\n",
      "-\n",
      "English:        forgive me\n",
      "Russian (true):  прости меня\n",
      "Russian (pred):  простите меня\n",
      "-\n",
      "English:        forgive us\n",
      "Russian (true):  прости нас\n",
      "Russian (pred):  простите нас\n",
      "-\n",
      "English:        forgive us\n",
      "Russian (true):  простите нас\n",
      "Russian (pred):  простите нас\n",
      "-\n",
      "English:        get a life\n",
      "Russian (true):  получи жизнь\n",
      "Russian (pred):  подорите\n",
      "-\n",
      "English:        get moving\n",
      "Russian (true):  \n",
      "Russian (pred):  вольси меня\n",
      "-\n",
      "English:        get moving\n",
      "Russian (true):  \n",
      "Russian (pred):  вольси меня\n",
      "-\n",
      "English:        get off me\n",
      "Russian (true):  отвали от меня\n",
      "Russian (pred):  идите сюда\n",
      "-\n",
      "English:        get to bed\n",
      "Russian (true):  иди спать\n",
      "Russian (pred):  постель\n",
      "-\n",
      "English:        get to bed\n",
      "Russian (true):  марш в постель\n",
      "Russian (pred):  постель\n",
      "-\n",
      "English:        get to bed\n",
      "Russian (true):  идите спать\n",
      "Russian (pred):  постель\n",
      "-\n",
      "English:        give it up\n",
      "Russian (true):  попустись\n",
      "Russian (pred):  \n",
      "-\n",
      "English:        go get tom\n",
      "Russian (true):  сходи за томом\n",
      "Russian (pred):  сходи за томом\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2100, 2120):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('English:       ', input_texts[seq_index])\n",
    "    print('Russian (true): ', target_texts[seq_index][1:-1])\n",
    "    print('Russian (pred): ', decoded_sentence[0:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Translate an English sentence to the target language\n",
    "\n",
    "1. Tokenization\n",
    "2. One-hot encode\n",
    "3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is: I love you\n",
      "translated sentence is: я люблю тебя\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'I love you'\n",
    "\n",
    "input_sequence = pad_sequences([[input_token_index.get(char) for char in input_sentence.lower()]], \n",
    "                               maxlen=max_encoder_seq_length, \n",
    "                               padding='post')\n",
    "\n",
    "input_x = onehot_encode(input_sequence, max_encoder_seq_length, 28)\n",
    "\n",
    "translated_sentence = decode_sequence(input_x)\n",
    "\n",
    "print('source sentence is: ' + input_sentence)\n",
    "print('translated sentence is: ' + translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the translation using BLEU score\n",
    "\n",
    "Reference: \n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://en.wikipedia.org/wiki/BLEU\n",
    "\n",
    "\n",
    "**Hint:** \n",
    "\n",
    "- Randomly partition the dataset to training, validation, and test. \n",
    "\n",
    "- Evaluate the BLEU score using the test set. Report the average.\n",
    "\n",
    "- A reasonable BLEU score should be 0.1 ~ 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(encoder_input_data)\n",
    "indices = numpy.random.permutation(n)\n",
    "encoder_input = encoder_input_data[indices]\n",
    "r_targets = numpy.array(target_texts)[indices]\n",
    "r_inputs = numpy.array(input_texts)[indices]\n",
    "train = int(n * .7)\n",
    "val = int(n * .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 15, 28), (4000, 15, 28), (6000, 15, 28))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = encoder_input[:train-val]\n",
    "y_train = r_targets[:train-val]\n",
    "train_eng = r_inputs[:train-val]\n",
    "\n",
    "X_val = encoder_input[train-val:train]\n",
    "y_val = r_targets[train-val:train]\n",
    "val_eng = r_inputs[train-val:train]\n",
    "\n",
    "X_test = encoder_input[train:]\n",
    "y_test = r_targets[train:]\n",
    "test_eng = r_inputs[train:]\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = numpy.zeros((input_seq.shape[0], num_decoder_tokens, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
       " array([[-4.27658595e-02, -1.04152277e-01,  1.28936380e-01,\n",
       "         -3.89424786e-02, -1.18907116e-01, -9.36928466e-02,\n",
       "         -4.14385572e-02,  1.83313414e-01, -4.02464066e-03,\n",
       "          1.30778715e-01,  3.67715918e-02, -3.18938717e-02,\n",
       "          4.00933577e-03,  3.94979231e-02,  2.63502891e-03,\n",
       "         -2.71826416e-01, -1.96826369e-01,  9.05111060e-02,\n",
       "          1.79608941e-01, -1.22693554e-01,  5.90306520e-01,\n",
       "         -3.62139225e-01,  5.68010628e-01,  6.50971457e-02,\n",
       "         -1.12470552e-01,  4.01371926e-01,  2.02626318e-01,\n",
       "          2.14823276e-01,  5.78474300e-03,  9.85996127e-02,\n",
       "         -2.47569427e-01, -2.12261409e-01,  5.96450984e-01,\n",
       "          1.71553224e-01, -3.45479697e-02, -6.07258715e-02,\n",
       "         -3.58708292e-01, -1.22728944e-02,  6.04622345e-03,\n",
       "         -2.75753111e-01,  2.08843395e-01, -6.37446577e-03,\n",
       "          4.78134640e-02,  4.72494155e-01,  1.16003416e-02,\n",
       "         -3.45793702e-02, -2.51616561e-03, -1.58314973e-01,\n",
       "         -1.11794062e-02, -8.15825239e-02, -5.44025153e-02,\n",
       "          1.52248107e-02,  2.20556203e-02, -4.77709472e-01,\n",
       "          1.56220824e-01,  4.21213657e-02, -3.51399511e-01,\n",
       "          4.28846804e-04, -9.14674178e-02, -2.72558182e-01,\n",
       "         -3.95932682e-02, -5.24791842e-03,  1.37440920e-01,\n",
       "          2.06428319e-01, -8.14462230e-02, -2.01702908e-01,\n",
       "          1.05617167e-02,  5.71436137e-02, -2.30681989e-02,\n",
       "         -3.14231008e-01,  8.46837312e-02,  1.47802457e-01,\n",
       "          1.14826530e-01,  1.56136928e-02, -2.64750898e-01,\n",
       "         -4.93683293e-02, -2.57222727e-02,  3.16580497e-02,\n",
       "          6.71781421e-01, -1.43344998e-01, -3.11394095e-01,\n",
       "          6.14915341e-02, -1.62038997e-01,  8.15882757e-02,\n",
       "          3.89706254e-01,  1.27169013e-01, -7.33516440e-02,\n",
       "          5.42147681e-02, -1.07889973e-01,  1.06927461e-03,\n",
       "          9.43006873e-02,  3.77628386e-01, -3.29381883e-01,\n",
       "         -1.60769299e-01, -5.23951575e-02, -6.18167687e-03,\n",
       "          3.39610018e-02, -1.25920221e-01, -1.28511414e-01,\n",
       "          1.90322384e-01, -5.19876853e-02, -6.20208941e-02,\n",
       "         -5.32758594e-01, -6.42182259e-03, -2.97962334e-02,\n",
       "         -2.43782207e-01,  3.73911788e-03,  4.12360989e-02,\n",
       "         -1.39421135e-01,  9.48608145e-02,  3.45228165e-02,\n",
       "         -5.37322909e-02,  3.60393912e-01, -1.14516720e-01,\n",
       "         -2.15283455e-03, -3.18599828e-02, -8.60370919e-02,\n",
       "          2.24497914e-01, -4.47506517e-01, -1.22620426e-02,\n",
       "          5.76268090e-03, -9.16830450e-02,  1.48940250e-01,\n",
       "         -2.92427093e-01, -6.59920990e-01,  1.06268182e-01,\n",
       "         -2.45820448e-01, -6.74570322e-01, -4.63983491e-02,\n",
       "         -2.76986122e-01,  2.89425682e-02, -2.45248359e-02,\n",
       "         -3.37681323e-01, -3.81163538e-01,  8.86874571e-02,\n",
       "         -5.89828705e-03, -2.67339885e-01, -3.94727401e-02,\n",
       "         -5.63773364e-02, -1.87225908e-01,  1.19149633e-01,\n",
       "          8.96218196e-02, -1.28652439e-01, -4.37243320e-02,\n",
       "          5.90631485e-01, -2.11664848e-02,  3.44588421e-02,\n",
       "         -3.50865312e-02,  4.76923771e-02, -2.33764231e-01,\n",
       "         -8.48177001e-02, -2.75649838e-02,  3.14963087e-02,\n",
       "          1.88471496e-01,  1.04769871e-01,  2.54275352e-01,\n",
       "         -4.74824935e-01, -8.44443068e-02,  4.84300517e-02,\n",
       "         -8.12951177e-02, -2.08363667e-01, -4.10710335e-01,\n",
       "          3.22343148e-02, -1.37219056e-01,  1.01034798e-01,\n",
       "          6.23881891e-02,  1.12037838e-01,  4.50728722e-02,\n",
       "          2.79336758e-02,  1.02606483e-01, -6.11884035e-02,\n",
       "         -1.86873078e-02,  7.13469833e-03, -1.14585482e-01,\n",
       "         -3.60325754e-01,  9.62418690e-02, -3.07757147e-02,\n",
       "         -1.76137850e-01, -2.28826422e-02,  1.16345827e-02,\n",
       "         -1.34015098e-01, -3.31163675e-01,  1.10915005e-02,\n",
       "         -1.40408352e-01, -5.47515303e-02,  4.23527556e-03,\n",
       "         -3.69087905e-01, -4.87023443e-02,  1.25746518e-01,\n",
       "          2.78813183e-01, -3.42930667e-02, -2.23816901e-01,\n",
       "          2.87597124e-02, -1.75989941e-02,  1.22460976e-01,\n",
       "         -4.40177433e-02, -1.43472269e-01,  3.39315161e-02,\n",
       "          3.76360752e-02, -3.92932594e-01,  2.74756074e-01,\n",
       "          9.77806225e-02, -2.48627766e-04, -8.45989306e-03,\n",
       "          1.16145000e-01, -2.52683073e-01,  2.07146019e-01,\n",
       "         -4.48727787e-01, -6.46486104e-01, -3.63384448e-02,\n",
       "          3.53631377e-02,  3.96981137e-03, -6.02836236e-02,\n",
       "          1.45470038e-01, -3.62041891e-01,  8.01599324e-02,\n",
       "          5.00546955e-02, -1.06589824e-01, -4.69654202e-02,\n",
       "         -1.31273314e-01, -1.01295605e-01,  3.18418145e-02,\n",
       "          5.50686964e-05,  1.02318935e-02,  2.82603472e-01,\n",
       "         -7.49929324e-02, -3.55920553e-01, -6.27070814e-02,\n",
       "          1.26940951e-01,  5.44828735e-03, -9.20025483e-02,\n",
       "          1.41129971e-01, -3.90260339e-01, -3.87539566e-01,\n",
       "          1.89821586e-01,  9.90864038e-02, -3.04638058e-01,\n",
       "          8.09590816e-01,  1.87646940e-01,  6.52946457e-02,\n",
       "          1.08461179e-01,  2.55925387e-01, -1.41272351e-01,\n",
       "         -1.02456369e-01, -4.52295355e-02,  6.67244988e-03,\n",
       "          1.32013410e-01, -2.98379809e-01,  1.31132171e-01,\n",
       "         -2.76151806e-01, -8.72518644e-02,  1.78675838e-02,\n",
       "          1.14998639e-01,  5.63428044e-01, -3.12262475e-01,\n",
       "         -7.06514763e-03,  2.52051324e-01, -5.48200071e-01,\n",
       "          1.14718461e-02,  1.08127177e-01,  2.15429217e-01,\n",
       "         -4.35546041e-01,  7.08831623e-02, -3.72328497e-02,\n",
       "         -3.53484243e-01,  5.57762682e-01, -2.32057691e-01,\n",
       "         -2.09389508e-01, -6.42229557e-01, -3.91870588e-02,\n",
       "          6.97462112e-02, -4.22637373e-01,  3.91762823e-01,\n",
       "          2.56058902e-01, -3.89865749e-02, -2.36872226e-01,\n",
       "         -4.49091457e-02, -2.07493588e-01,  6.42368197e-01,\n",
       "         -5.65073073e-01,  3.41442153e-02, -1.16763167e-01,\n",
       "          5.30506015e-01, -2.79478908e-01, -1.13189362e-01,\n",
       "         -1.24108300e-01,  5.46340086e-02, -2.36605793e-01,\n",
       "         -2.13077813e-02, -2.41300896e-01,  2.85612375e-01,\n",
       "         -4.63963360e-01, -4.84929591e-01,  1.83191940e-01,\n",
       "         -3.31470430e-01, -1.06744140e-01,  8.15805495e-02,\n",
       "         -3.20005417e-01, -5.14913723e-02,  3.99777859e-01,\n",
       "         -1.36035636e-01, -3.36753547e-01,  2.60536194e-01,\n",
       "         -1.42166391e-01,  4.07818884e-01, -1.71343774e-01,\n",
       "          3.21807981e-01, -1.24876000e-01,  3.66655678e-01,\n",
       "          8.50280896e-02, -5.06018177e-02,  7.28884935e-01,\n",
       "          8.50861311e-01, -4.74976450e-01, -3.49179834e-01,\n",
       "          1.30498158e-02,  8.73131335e-01, -5.36121577e-02,\n",
       "         -3.81801873e-01, -3.49616468e-01,  4.04624552e-01,\n",
       "          3.60784531e-01,  2.60721724e-02, -1.14320569e-01,\n",
       "         -6.11624479e-01, -5.81879139e-01,  7.82282829e-01,\n",
       "          2.12351352e-01, -2.16532439e-01, -4.13157225e-01,\n",
       "          1.17328763e-02,  8.87614489e-02,  1.32857203e-01,\n",
       "         -4.97595280e-01, -6.80168033e-01, -3.03243343e-02,\n",
       "         -1.20523968e-03,  2.23640084e-01, -6.49642825e-01,\n",
       "         -1.04154669e-01, -9.28427950e-02,  3.42288554e-01,\n",
       "         -5.67727536e-02, -2.67031670e-01, -4.28321928e-01,\n",
       "         -3.91498834e-01,  5.50777614e-01,  5.62389612e-01,\n",
       "          1.16195969e-01, -5.21275662e-02, -2.30100289e-01,\n",
       "          3.95344645e-01,  8.36435318e-01,  9.25398618e-02,\n",
       "         -1.96235359e-01,  1.50696635e-01, -5.20677455e-02,\n",
       "          7.03112066e-01,  2.58511186e-01, -4.29064929e-01,\n",
       "         -3.51540744e-01, -2.90645733e-02,  4.07906055e-01,\n",
       "          3.95793058e-02,  5.46663161e-03, -1.83320269e-01,\n",
       "          1.94136605e-01,  1.15794264e-01, -1.80272296e-01,\n",
       "          1.49664395e-02, -2.67602116e-01, -2.02159822e-01,\n",
       "         -1.67343710e-02,  2.45089486e-01,  1.49746612e-01,\n",
       "          3.00494194e-01, -7.21032619e-02,  1.59090742e-01,\n",
       "          7.33983397e-01, -5.76754333e-03,  1.55806998e-02,\n",
       "          3.85107934e-01,  3.44968140e-01, -4.25073475e-01,\n",
       "         -5.91310799e-01, -3.45964134e-01, -4.66443956e-01,\n",
       "         -2.12596282e-01,  5.09785473e-01,  3.85793120e-01,\n",
       "         -4.76520002e-01, -9.54881310e-01, -2.56090015e-01,\n",
       "          1.19611444e-02, -4.43969399e-01, -1.58948507e-02,\n",
       "         -2.28512481e-01,  4.95939627e-02,  2.95778930e-01,\n",
       "         -3.07865411e-01,  2.51008600e-01,  2.07191199e-01,\n",
       "          1.84860975e-01,  7.45956931e-05, -4.52633142e-01,\n",
       "         -1.04757957e-01, -4.09878850e-01,  8.35254550e-01,\n",
       "         -3.55666727e-02,  4.29844052e-01,  2.23358721e-01,\n",
       "          6.16759472e-02, -2.47810185e-01, -3.75566855e-02,\n",
       "          7.38139510e-01, -4.88780379e-01,  1.16211005e-01,\n",
       "          7.65766129e-02,  2.85892457e-01,  2.86205798e-01,\n",
       "          5.41142635e-02, -5.25615990e-01, -5.98734975e-01,\n",
       "         -4.62222517e-01,  6.13987088e-01, -5.97572088e-01,\n",
       "          6.95487112e-02,  2.61323571e-01, -1.27111658e-01,\n",
       "         -2.84549922e-01,  3.96596938e-02,  4.15290833e-01,\n",
       "         -1.03314467e-01,  8.26360062e-02, -2.12487400e-01,\n",
       "         -7.53318608e-01, -1.78207338e-01, -2.80492902e-01,\n",
       "         -8.55572581e-01, -4.97193821e-02,  9.19339433e-03,\n",
       "         -6.56126887e-02, -4.24915642e-01, -3.61832619e-01,\n",
       "          2.04658434e-02, -7.36564517e-01,  7.44245499e-02,\n",
       "         -8.91626358e-01,  4.70699519e-01,  9.34844613e-02,\n",
       "         -2.02795386e-01,  6.54419810e-02, -9.12773728e-01,\n",
       "          3.68893802e-01,  2.88317382e-01, -4.79746237e-02,\n",
       "         -4.67717171e-01, -6.96128964e-01,  5.63161969e-02,\n",
       "         -6.24728918e-01,  5.22432802e-03,  6.48901612e-02,\n",
       "          1.18659884e-01, -3.49769294e-01, -1.13969848e-01,\n",
       "          6.09077871e-01, -5.49640715e-01, -1.81067422e-01,\n",
       "         -2.64609843e-01, -7.49890059e-02,  5.44884562e-01,\n",
       "         -1.19604163e-01, -3.52778286e-01, -4.86661017e-01,\n",
       "         -1.24841884e-01,  1.53455064e-02,  1.98581189e-01,\n",
       "         -9.44299251e-02, -2.18886748e-01,  1.96427524e-01,\n",
       "          6.98540270e-01, -3.65554839e-01, -6.20537937e-01,\n",
       "          1.02698766e-01,  3.03856313e-01,  5.91413796e-01,\n",
       "         -2.18547061e-01,  8.97791922e-01,  6.34003878e-01,\n",
       "          1.01271316e-01,  3.72368366e-01, -4.06175822e-01,\n",
       "          2.59611994e-01,  1.02025583e-01,  5.96444011e-01,\n",
       "          1.12354860e-01, -4.64160591e-01,  1.48467859e-03,\n",
       "         -8.74717459e-02, -2.21136317e-01, -7.19316363e-01,\n",
       "         -7.94402540e-01, -4.23917681e-01, -4.30998430e-02,\n",
       "          1.00024134e-01, -4.17872101e-01, -1.60547256e-01,\n",
       "          3.84212404e-01,  2.31233407e-02,  7.92715967e-01,\n",
       "          9.97869112e-03,  2.31865663e-02]], dtype=float32),\n",
       " array([[-2.21764162e-01, -1.00022030e+00,  4.16183501e-01,\n",
       "         -1.34932935e-01, -2.50502646e-01, -7.21946537e-01,\n",
       "         -2.85932809e-01,  1.00742674e+00, -9.08719935e-03,\n",
       "          1.94278151e-01,  2.78814703e-01, -2.05291480e-01,\n",
       "          1.94516908e-02,  9.29243118e-02,  5.17209107e-03,\n",
       "         -9.86802757e-01, -4.40527320e-01,  2.70631462e-01,\n",
       "          7.46065378e-01, -3.39556098e-01,  1.34304571e+00,\n",
       "         -7.64725208e-01,  2.71919847e+00,  3.25232595e-01,\n",
       "         -3.70023638e-01,  8.85844588e-01,  1.10204661e+00,\n",
       "          1.47415841e+00,  2.41149981e-02,  2.08936751e-01,\n",
       "         -5.53853154e-01, -2.89789081e-01,  1.26870966e+00,\n",
       "          2.55896568e-01, -3.25732112e-01, -4.17640090e-01,\n",
       "         -5.99307120e-01, -8.36235657e-02,  5.97078986e-02,\n",
       "         -7.80799270e-01,  8.14599097e-01, -1.81741640e-02,\n",
       "          1.92278624e-01,  1.67355800e+00,  4.56880294e-02,\n",
       "         -1.27687022e-01, -1.94842946e-02, -9.41291213e-01,\n",
       "         -8.39208141e-02, -7.13380635e-01, -2.79780447e-01,\n",
       "          1.84466094e-01,  3.44954506e-02, -1.00412810e+00,\n",
       "          1.98147759e-01,  1.32240906e-01, -3.13854766e+00,\n",
       "          5.20969217e-04, -1.46572664e-01, -6.69852793e-01,\n",
       "         -3.63758057e-01, -3.44936624e-02,  9.90118027e-01,\n",
       "          4.14790899e-01, -3.93542796e-01, -5.21485925e-01,\n",
       "          2.89891791e-02,  1.74770951e-01, -3.94908041e-01,\n",
       "         -2.91482329e+00,  1.24554083e-01,  1.98975575e+00,\n",
       "          4.05492783e-01,  5.04462048e-02, -2.60701871e+00,\n",
       "         -2.34570831e-01, -2.50703454e-01,  8.10855255e-02,\n",
       "          1.48790944e+00, -3.67546916e-01, -4.60477889e-01,\n",
       "          1.02068365e+00, -7.78962731e-01,  2.53109694e-01,\n",
       "          6.01888657e-01,  2.60498315e-01, -3.08561772e-01,\n",
       "          6.95451722e-02, -3.87254119e-01,  1.55613571e-03,\n",
       "          2.16337293e-01,  2.08747840e+00, -1.48300588e+00,\n",
       "         -3.68965536e-01, -1.95828855e-01, -9.94084775e-02,\n",
       "          8.99507776e-02, -3.62972409e-01, -1.23955810e+00,\n",
       "          9.95061934e-01, -1.34902984e-01, -7.82283902e-01,\n",
       "         -1.39348185e+00, -1.51234558e-02, -9.64210555e-02,\n",
       "         -6.58479452e-01,  2.79676653e-02,  3.13224584e-01,\n",
       "         -4.64208543e-01,  2.25450844e-01,  1.14321746e-01,\n",
       "         -1.26835689e-01,  7.06748307e-01, -3.57812792e-01,\n",
       "         -1.94340907e-02, -1.39953718e-01, -4.33107585e-01,\n",
       "          7.13340461e-01, -1.15121007e+00, -9.93280932e-02,\n",
       "          6.87901080e-02, -2.08933562e-01,  3.82089287e-01,\n",
       "         -8.03452730e-01, -1.02347600e+00,  4.85626131e-01,\n",
       "         -9.06054795e-01, -3.43163228e+00, -2.70037085e-01,\n",
       "         -1.36808407e+00,  1.18284978e-01, -6.79114386e-02,\n",
       "         -4.75446582e-01, -7.33910024e-01,  2.28650540e-01,\n",
       "         -7.67001882e-02, -4.64971036e-01, -1.20479512e+00,\n",
       "         -2.03643203e+00, -3.62464094e+00,  1.87631533e-01,\n",
       "          4.27431256e-01, -2.81008184e-01, -2.02646598e-01,\n",
       "          1.48720229e+00, -7.14166164e-02,  2.25753456e-01,\n",
       "         -8.43698382e-02,  1.42142832e-01, -3.28569502e-01,\n",
       "         -3.81397426e-01, -2.33577803e-01,  1.23385333e-01,\n",
       "          4.20662701e-01,  1.25651467e+00,  4.76762146e-01,\n",
       "         -7.73418367e-01, -2.16016963e-01,  3.05983245e-01,\n",
       "         -2.96868920e-01, -6.14006639e-01, -4.01166964e+00,\n",
       "          1.17226422e-01, -1.65887222e-01,  4.92679358e-01,\n",
       "          1.05739243e-01,  2.08981857e-01,  9.55798551e-02,\n",
       "          1.49444088e-01,  6.21293545e-01, -5.11492372e-01,\n",
       "         -8.45802352e-02,  5.24024405e-02, -3.34241867e-01,\n",
       "         -1.22469902e+00,  6.50602281e-01, -1.25585526e-01,\n",
       "         -4.40455109e-01, -2.87793517e-01,  7.36193806e-02,\n",
       "         -5.48863590e-01, -1.32835877e+00,  1.04048483e-01,\n",
       "         -4.92618740e-01, -2.17911929e-01,  3.50548141e-02,\n",
       "         -5.52727795e+00, -4.04214233e-01,  2.00815439e-01,\n",
       "          8.95998061e-01, -3.19606841e-01, -1.03418028e+00,\n",
       "          2.39514768e-01, -1.78163379e-01,  3.98585558e-01,\n",
       "         -7.88696587e-01, -7.51272857e-01,  2.02061862e-01,\n",
       "          2.21345961e-01, -2.72348499e+00,  4.19548452e-01,\n",
       "          2.37395376e-01, -7.35111069e-04, -2.20316440e-01,\n",
       "          5.04418194e-01, -9.77928638e-01,  2.73986369e-01,\n",
       "         -7.49137163e-01, -2.05729508e+00, -2.12435693e-01,\n",
       "          2.09570527e-01,  3.09958532e-02, -9.42321271e-02,\n",
       "          2.58342534e-01, -2.84313941e+00,  1.79232851e-01,\n",
       "          9.28926468e-02, -1.82307470e+00, -1.80506945e-01,\n",
       "         -8.35834742e-01, -3.40990961e-01,  1.26878023e-01,\n",
       "          5.15690073e-04,  2.34178398e-02,  4.53880161e-01,\n",
       "         -1.29589963e+00, -8.88163030e-01, -2.04128474e-01,\n",
       "          3.97427678e-01,  9.61637776e-03, -2.20980152e-01,\n",
       "          7.59599030e-01, -7.46262610e-01, -8.20181787e-01,\n",
       "          5.15783370e-01,  2.25751966e-01, -6.26201212e-01,\n",
       "          2.47243309e+00,  3.50642741e-01,  3.29635561e-01,\n",
       "          2.27824450e-01,  7.81676710e-01, -9.30964351e-01,\n",
       "         -3.51010352e-01, -5.85375547e-01,  1.22242728e-02,\n",
       "          4.75155592e-01, -8.76207769e-01,  2.51759797e-01,\n",
       "         -9.29761171e-01, -3.99273932e-01,  3.67758274e-02,\n",
       "          3.04426223e-01,  8.05968523e-01, -1.69957507e+00,\n",
       "         -6.79852664e-02,  2.63593346e-01, -4.27314281e+00,\n",
       "          2.12902445e-02,  1.07635438e+00,  4.11027282e-01,\n",
       "         -6.89573348e-01,  3.47155929e-01, -4.83962059e+00,\n",
       "         -1.07384503e+00,  6.51130676e-01, -1.67375660e+00,\n",
       "         -4.52689052e-01, -1.02901506e+00, -8.73280942e-01,\n",
       "          2.60201156e-01, -6.71265721e-01,  4.62137431e-01,\n",
       "          3.22621912e-01, -5.92743792e-02, -2.91827500e-01,\n",
       "         -5.45105577e-01, -5.14774442e-01,  1.32127368e+00,\n",
       "         -7.10599244e-01,  3.90711986e-02, -8.45045388e-01,\n",
       "          7.82107294e-01, -6.55123234e-01, -1.61750659e-01,\n",
       "         -4.72126126e-01,  5.91059290e-02, -1.73427975e+00,\n",
       "         -5.58262527e-01, -1.00457227e+00,  1.70130539e+00,\n",
       "         -9.66413498e-01, -5.63750863e-01,  5.44646800e-01,\n",
       "         -7.95506060e-01, -1.32983074e-01,  5.14024591e+00,\n",
       "         -3.56208414e-01, -4.41496849e-01,  1.08834183e+00,\n",
       "         -1.49788484e-01, -5.69993794e-01,  3.91226888e-01,\n",
       "         -2.16949314e-01,  1.95250535e+00, -8.48509550e-01,\n",
       "          6.39975190e-01, -1.49476901e-01,  7.40721464e-01,\n",
       "          1.44888723e+00, -1.47461534e-01,  1.17664170e+00,\n",
       "          1.40841937e+00, -7.70884097e-01, -3.26549530e+00,\n",
       "          3.70909005e-01,  1.42298579e+00, -1.15728676e-01,\n",
       "         -8.63490880e-01, -1.04344964e+00,  1.13417530e+00,\n",
       "          4.14440393e-01,  2.89102867e-02, -1.42952359e+00,\n",
       "         -8.28490317e-01, -7.27882802e-01,  1.39842820e+00,\n",
       "          3.22132200e-01, -2.54237652e-01, -5.34616828e-01,\n",
       "          1.57291830e+00,  1.20486401e-01,  1.47431418e-01,\n",
       "         -1.13510740e+00, -2.26439524e+00, -1.69426942e+00,\n",
       "         -1.95854083e-02,  4.27728206e-01, -7.82995343e-01,\n",
       "         -1.77501664e-01, -9.45738256e-01,  7.66808450e-01,\n",
       "         -4.99003559e-01, -5.44495821e-01, -6.08260691e-01,\n",
       "         -1.06264627e+00,  8.30369413e-01,  7.09861040e-01,\n",
       "          4.26664472e-01, -8.79024863e-02, -3.23506570e+00,\n",
       "          1.80637360e+00,  2.59348464e+00,  2.86417753e-01,\n",
       "         -2.66847730e-01,  3.27218294e-01, -1.43431640e+00,\n",
       "          1.04409242e+00,  3.55987310e-01, -5.18197894e-01,\n",
       "         -8.45159769e-01, -3.32669839e-02,  6.29736543e-01,\n",
       "          1.06786512e-01,  2.89597902e-02, -1.77113640e+00,\n",
       "          1.12641597e+00,  2.55270433e+00, -6.68175995e-01,\n",
       "          1.66193470e-01, -5.96998394e-01, -5.90465248e-01,\n",
       "         -2.29517408e-02,  8.77409816e-01,  1.28237045e+00,\n",
       "          4.75411266e-01, -8.60129744e-02,  8.12053025e-01,\n",
       "          9.73177969e-01, -2.57133335e-01,  2.05962732e-02,\n",
       "          9.21175897e-01,  3.73866963e+00, -8.83188307e-01,\n",
       "         -6.93013668e-01, -5.21126390e-01, -1.68075597e+00,\n",
       "         -3.54953587e-01,  6.27004147e-01,  6.74196601e-01,\n",
       "         -5.30907512e-01, -2.01671433e+00, -2.66501755e-01,\n",
       "          1.22600305e+00, -5.84845543e-01, -5.73932350e-01,\n",
       "         -5.68517268e-01,  6.39594942e-02,  5.21143913e-01,\n",
       "         -6.32575929e-01,  4.19231921e-01,  5.52182794e-01,\n",
       "          2.21283257e-01,  4.21400787e-03, -1.42024350e+00,\n",
       "         -5.69521546e-01, -4.65510845e-01,  1.34930599e+00,\n",
       "         -2.57453531e-01,  5.87540388e-01,  2.58182704e-01,\n",
       "          5.69835007e-01, -4.74322081e-01, -1.06706604e-01,\n",
       "          1.10063887e+00, -7.15531349e-01,  1.68614283e-01,\n",
       "          8.48391429e-02,  1.11308372e+00,  1.39450455e+00,\n",
       "          6.39333278e-02, -7.27389872e-01, -1.09260488e+00,\n",
       "         -7.14354396e-01,  7.47557938e-01, -3.23245788e+00,\n",
       "          1.32068858e-01,  5.22876918e-01, -4.60677683e-01,\n",
       "         -4.84817457e+00,  6.68861195e-02,  6.05288923e-01,\n",
       "         -1.25459683e+00,  5.23342907e-01, -1.12981546e+00,\n",
       "         -1.23579538e+00, -3.39466333e+00, -6.00885212e-01,\n",
       "         -1.34122455e+00, -1.39724657e-01,  2.73113310e-01,\n",
       "         -3.54287767e+00, -8.10572743e-01, -4.35512125e-01,\n",
       "          3.19922157e-02, -2.54015064e+00,  1.82594693e+00,\n",
       "         -1.97670007e+00,  7.68964887e-01,  3.31816643e-01,\n",
       "         -3.26958805e-01,  1.13582063e+00, -2.30378151e+00,\n",
       "          4.00620759e-01,  3.57029438e-01, -4.47197258e-01,\n",
       "         -5.72535038e-01, -9.66103137e-01,  8.05724561e-02,\n",
       "         -7.40659058e-01,  9.20065958e-03,  2.60647744e-01,\n",
       "          2.38657069e+00, -3.90199482e-01, -5.21960199e-01,\n",
       "          9.39973891e-01, -6.62540674e-01, -2.53137261e-01,\n",
       "         -5.74504197e-01, -2.93812752e-01,  1.16564810e+00,\n",
       "         -7.53878415e-01, -5.04016161e-01, -7.45656967e-01,\n",
       "         -5.34478426e-01,  1.89757161e-02,  5.30989110e-01,\n",
       "         -1.36770040e-01, -3.17439497e-01,  5.08122444e-01,\n",
       "          1.29263747e+00, -8.79955828e-01, -9.89076257e-01,\n",
       "          8.19581032e-01,  3.98063207e+00,  7.91337669e-01,\n",
       "         -4.13928658e-01,  1.48605585e+00,  9.46484923e-01,\n",
       "          2.06425175e-01,  5.14444232e-01, -7.79537797e-01,\n",
       "          1.73597217e+00,  4.83395964e-01,  1.01399159e+00,\n",
       "          2.67288625e-01, -7.26058543e-01,  7.54654082e-03,\n",
       "         -1.06901392e-01, -3.13156813e-01, -9.21033442e-01,\n",
       "         -1.25571954e+00, -1.79678023e+00, -1.47740975e-01,\n",
       "          9.72505748e-01, -6.02780282e-01, -1.20589149e+00,\n",
       "          5.81165016e-01,  1.52933493e-01,  1.37606227e+00,\n",
       "          5.48490658e-02,  8.03708434e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_value = encoder_model.predict(X_test[0:1])\n",
    "target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "[target_seq] + states_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4812\u001b[0m     \u001b[0;31m# without introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4813\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4814\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-2d2fc0b34b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# for trying out decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6abcb6307eab>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# this line of code is greedy selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[1;32m   1597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1599\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    385\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     dataset = dataset.map(\n\u001b[0m\u001b[1;32m    388\u001b[0m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1805\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m       return ParallelMapDataset(\n\u001b[0m\u001b[1;32m   1808\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4241\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4242\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4243\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3049\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[0;32m-> 3051\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   3052\u001b[0m         *args, **kwargs)\n\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3517\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3453\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3454\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3455\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mgrab_batch\u001b[0;34m(i, data)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrab_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     dataset = dataset.map(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrab_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     dataset = dataset.map(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4813\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4814\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4815\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     \u001b[0mbatch_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m   \u001b[0mbatch_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch_dims\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   3801\u001b[0m         \u001b[0;34m\"GatherV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3802\u001b[0m                     batch_dims=batch_dims, name=name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_Flatten\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;34m\"\"\"Converts [1, 2, [3, 4], [5]] to [1, 2, 3, 4, 5].\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;31m# [1, 2, [3, 4], [5]] -> [[1], [2], [3, 4], [5]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m   \u001b[0ml_of_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m   \u001b[0;31m# [[1], [2], [3, 4], [5]] -> [1, 2, 3, 4, 5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_of_l\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;34m\"\"\"Converts [1, 2, [3, 4], [5]] to [1, 2, 3, 4, 5].\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;31m# [1, 2, [3, 4], [5]] -> [[1], [2], [3, 4], [5]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m   \u001b[0ml_of_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m   \u001b[0;31m# [[1], [2], [3, 4], [5]] -> [1, 2, 3, 4, 5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_of_l\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bleu = list()\n",
    "for seq_index in range(X_test.shape[0]):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = X_test[seq_index: seq_index+1]\n",
    "    pred = decode_sequence(input_seq)[0:-1]\n",
    "    real = y_test[seq_index][1:-1]\n",
    "    \n",
    "    pair = ([real.split()], pred.split())\n",
    "    bleu.append(pair)\n",
    "\n",
    "bleu[2000:2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42963946527586006"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores = [sentence_bleu(real, pred, (1,0,0,0)) for real, pred in bleu]\n",
    "sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
